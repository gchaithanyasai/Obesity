{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 13491801,
          "sourceType": "datasetVersion",
          "datasetId": 8566185
        }
      ],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Obesity_pred_v4.0",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "rnK3BlinXC0A"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "shrinithiandalt_obesity_dataset1_path = kagglehub.dataset_download('shrinithiandalt/obesity-dataset1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "dwamyMu7XC0K"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-27T19:20:04.882779Z",
          "iopub.execute_input": "2025-10-27T19:20:04.883036Z",
          "iopub.status.idle": "2025-10-27T19:20:06.104136Z",
          "shell.execute_reply.started": "2025-10-27T19:20:04.883015Z",
          "shell.execute_reply": "2025-10-27T19:20:06.102825Z"
        },
        "id": "XBIFbzKZXC0L"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# OBESITY RISK FACTOR ANALYSIS & MODEL PREDICTION\n",
        "# ============================================\n",
        "# Single script (college-project style)\n",
        "# - PCA block (standalone) added BEFORE preprocessing/modeling\n",
        "# - PCA is ANALYTICAL ONLY and does NOT change the model pipeline\n",
        "# - Only correlation heatmap is kept from EDA (on encoded features)\n",
        "# - No clustering; model logic preserved exactly\n",
        "# ============================================\n",
        "\n",
        "# -----------------------------\n",
        "# 0. LIBRARIES\n",
        "# -----------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# -----------------------------\n",
        "# ðŸ“Œ CHANGES MADE (vs original code)\n",
        "# -----------------------------\n",
        "# 1. Added standalone PCA analysis block BEFORE preprocessing/modeling (PCA for analysis only).\n",
        "# 2. Kept all model training/tuning/cv logic unchanged.\n",
        "# 3. Removed clustering and any other EDA plots; kept only correlation heatmap (on encoded features).\n",
        "# 4. Reworked variable names to student-friendly style (df_train, x_scaled, best_xgb, etc.).\n",
        "# 5. PCA uses its own preprocessing pipeline (imputer + scaler + one-hot) and prints explained variance.\n",
        "# Note: PCA results are printed and plotted but NOT used in model training.\n",
        "\n",
        "# -----------------------------\n",
        "# 1. LOAD DATA (train1, train2, test)\n",
        "# -----------------------------\n",
        "df_train1 = pd.read_csv(\"/kaggle/input/obesity-dataset1/train1.csv\")\n",
        "df_train2 = pd.read_csv(\"/kaggle/input/obesity-dataset1/train2.csv\")\n",
        "df_test = pd.read_csv(\"/kaggle/input/obesity-dataset1/test.csv\")\n",
        "\n",
        "print(\"train1 shape:\", df_train1.shape)\n",
        "print(\"train2 shape:\", df_train2.shape)\n",
        "print(\"test shape :\", df_test.shape)\n",
        "\n",
        "# Combine the two training parts (this combined set will be used both for PCA analysis and for modeling)\n",
        "combined_train = pd.concat([df_train1, df_train2], axis=0, ignore_index=True)\n",
        "print(\"combined_train shape:\", combined_train.shape)\n",
        "\n",
        "# ====================================================\n",
        "# PART A â€” PCA ANALYSIS (Standalone, BEFORE modeling)\n",
        "# This block performs its own preprocessing inside a pipeline.\n",
        "# It does NOT alter combined_train or any subsequent modeling data.\n",
        "# ====================================================\n",
        "print(\"\\n=== PCA ANALYSIS (Standalone) ===\")\n",
        "\n",
        "# Prepare feature list for PCA (drop id and target)\n",
        "target_col = 'WeightCategory'\n",
        "pca_feature_cols = [c for c in combined_train.columns if c not in ['id', target_col]]\n",
        "X_pca_raw = combined_train[pca_feature_cols].copy()\n",
        "\n",
        "# Identify numeric and categorical columns\n",
        "numeric_cols_pca = X_pca_raw.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_cols_pca = X_pca_raw.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(\"Numeric cols for PCA:\", numeric_cols_pca)\n",
        "print(\"Categorical cols for PCA:\", categorical_cols_pca)\n",
        "\n",
        "# Build preprocessing pipelines for PCA (impute + scale numeric, impute + onehot categorical)\n",
        "num_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "cat_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    # OneHotEncoder with drop='first' to avoid full rank and handle_unknown='ignore'\n",
        "    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preprocessor_pca = ColumnTransformer(transformers=[\n",
        "    ('num', num_pipeline, numeric_cols_pca),\n",
        "    ('cat', cat_pipeline, categorical_cols_pca)\n",
        "])\n",
        "\n",
        "# PCA pipeline: preprocessor then PCA (keep all components initially)\n",
        "pca_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor_pca),\n",
        "    ('pca', PCA(n_components=None))\n",
        "])\n",
        "\n",
        "# Fit PCA pipeline on the PCA feature matrix (this will not affect model pipeline)\n",
        "X_pca_transformed = pca_pipeline.fit_transform(X_pca_raw)\n",
        "\n",
        "# Extract explained variance ratios\n",
        "pca_model = pca_pipeline.named_steps['pca']\n",
        "explained_var = pca_model.explained_variance_ratio_\n",
        "cum_explained_var = np.cumsum(explained_var)\n",
        "\n",
        "# Print explained variance for each PC\n",
        "print(\"\\nExplained variance ratio by principal component (PCA):\")\n",
        "for i, var in enumerate(explained_var, start=1):\n",
        "    print(f\"PC{i}: {var:.4f}\")\n",
        "\n",
        "# Print cumulative explained variance and show elbow-ish info\n",
        "print(\"\\nCumulative explained variance (first 20 shown or all if less):\")\n",
        "for i, cumv in enumerate(cum_explained_var[:min(20, len(cum_explained_var))], start=1):\n",
        "    print(f\"PC{i}: Cumulative = {cumv:.4f}\")\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(cum_explained_var) + 1), cum_explained_var, marker='o')\n",
        "plt.title(\"PCA - Cumulative Explained Variance (Standalone Analysis)\")\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance Ratio\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"=== End of PCA Analysis (standalone) ===\\n\")\n",
        "\n",
        "# ====================================================\n",
        "# PART B â€” PREPROCESSING & CORRELATION (for modeling)\n",
        "# ====================================================\n",
        "# The following encoding is the same approach used in the original script.\n",
        "# We will encode categorical features in train & test, then show correlation heatmap\n",
        "# on encoded features (this heatmap is the only EDA plot requested).\n",
        "# ====================================================\n",
        "\n",
        "# Copy combined_train to df_train for modeling steps (do not modify combined_train used earlier)\n",
        "df_train = combined_train.copy()\n",
        "\n",
        "# Label-encode the target column for modeling\n",
        "le_target = LabelEncoder()\n",
        "df_train['WeightCategory'] = le_target.fit_transform(df_train['WeightCategory'])\n",
        "\n",
        "# Helper: encode object columns with LabelEncoder (keeps encoders for test mapping)\n",
        "def encode_features(df, encoders=None):\n",
        "    encoders = encoders or {}\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object' and col != 'WeightCategory':\n",
        "            if col not in encoders:\n",
        "                le = LabelEncoder()\n",
        "                df[col] = le.fit_transform(df[col].astype(str))\n",
        "                encoders[col] = le\n",
        "            else:\n",
        "                le = encoders[col]\n",
        "                # If unseen category appears in test, map to -1\n",
        "                df[col] = df[col].map(lambda x: le.transform([x])[0] if x in le.classes_ else -1)\n",
        "    return df, encoders\n",
        "\n",
        "# Encode train features and test features using the same encoders\n",
        "df_train, feature_encoders = encode_features(df_train)\n",
        "df_test_enc, _ = encode_features(df_test.copy(), feature_encoders)  # operate on a copy for safety\n",
        "\n",
        "# Correlation heatmap on encoded features (drop id & target)\n",
        "feature_cols_encoded = [c for c in df_train.columns if c not in ['id', 'WeightCategory']]\n",
        "corr_matrix = df_train[feature_cols_encoded].corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\n",
        "plt.title(\"Correlation Heatmap (Encoded Features)\")\n",
        "plt.show()\n",
        "\n",
        "# ====================================================\n",
        "# PART C â€” MODEL PREPARATION (Scaling) and MODELING (unchanged logic)\n",
        "# ====================================================\n",
        "# Prepare X, y for modeling\n",
        "X = df_train.drop(['id', 'WeightCategory'], axis=1)\n",
        "y = df_train['WeightCategory']\n",
        "X_test = df_test_enc.drop(['id'], axis=1)\n",
        "\n",
        "# Scaling (explicit)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Cross-validation strategy (same as original)\n",
        "cv_strategy = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
        "\n",
        "# -----------------------------\n",
        "# MODEL 1: Logistic Regression (tuning)\n",
        "# -----------------------------\n",
        "lr_param_grid = {\n",
        "    'C': np.logspace(-3, 2, 10),\n",
        "    'solver': ['lbfgs', 'saga'],\n",
        "    'max_iter': [500, 1000]\n",
        "}\n",
        "lr_model = LogisticRegression(multi_class='multinomial', random_state=42)\n",
        "lr_tuner = RandomizedSearchCV(\n",
        "    lr_model,\n",
        "    lr_param_grid,\n",
        "    n_iter=20,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n--- Tuning Logistic Regression ---\")\n",
        "lr_tuner.fit(X_scaled, y)\n",
        "best_lr = lr_tuner.best_estimator_\n",
        "acc_lr = np.mean(cross_val_score(best_lr, X_scaled, y, cv=cv_strategy, scoring='accuracy'))\n",
        "\n",
        "# -----------------------------\n",
        "# MODEL 2: Random Forest (tuning)\n",
        "# -----------------------------\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200, 300, 500],\n",
        "    'max_depth': [5, 10, 15, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_tuner = RandomizedSearchCV(\n",
        "    rf_model,\n",
        "    rf_param_grid,\n",
        "    n_iter=30,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n--- Tuning Random Forest ---\")\n",
        "rf_tuner.fit(X_scaled, y)\n",
        "best_rf = rf_tuner.best_estimator_\n",
        "acc_rf = np.mean(cross_val_score(best_rf, X_scaled, y, cv=cv_strategy, scoring='accuracy'))\n",
        "\n",
        "# -----------------------------\n",
        "# MODEL 3: AdaBoost (tuning)\n",
        "# -----------------------------\n",
        "ada_param_grid = {\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0]\n",
        "}\n",
        "ada_model = AdaBoostClassifier(random_state=42)\n",
        "ada_tuner = RandomizedSearchCV(\n",
        "    ada_model,\n",
        "    ada_param_grid,\n",
        "    n_iter=20,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n--- Tuning AdaBoost ---\")\n",
        "ada_tuner.fit(X_scaled, y)\n",
        "best_ada = ada_tuner.best_estimator_\n",
        "acc_ada = np.mean(cross_val_score(best_ada, X_scaled, y, cv=cv_strategy, scoring='accuracy'))\n",
        "\n",
        "# -----------------------------\n",
        "# MODEL 4: XGBoost (deep tuning)\n",
        "# -----------------------------\n",
        "print(\"\\nðŸ”§ Starting Deep Hyperparameter Tuning on XGBoost (with Refined CV)...\")\n",
        "\n",
        "xgb_param_grid = {\n",
        "    'n_estimators': [400, 600, 800, 1000],\n",
        "    'max_depth': [4, 6, 8, 10],\n",
        "    'learning_rate': [0.005, 0.01, 0.03, 0.05],\n",
        "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
        "    'gamma': [0, 0.1, 0.3, 0.5],\n",
        "    'min_child_weight': [1, 3, 5],\n",
        "    'reg_lambda': [1, 1.5, 2, 3],\n",
        "    'reg_alpha': [0, 0.1, 0.3, 0.5]\n",
        "}\n",
        "\n",
        "xgb_base = XGBClassifier(\n",
        "    random_state=42,\n",
        "    eval_metric='mlogloss',\n",
        "    tree_method='hist',\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "xgb_tuner = RandomizedSearchCV(\n",
        "    estimator=xgb_base,\n",
        "    param_distributions=xgb_param_grid,\n",
        "    n_iter=50,\n",
        "    scoring='accuracy',\n",
        "    cv=cv_strategy,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\n--- Tuning XGBoost ---\")\n",
        "xgb_tuner.fit(X_scaled, y)\n",
        "best_xgb = xgb_tuner.best_estimator_\n",
        "acc_xgb = np.mean(cross_val_score(best_xgb, X_scaled, y, cv=cv_strategy, scoring='accuracy'))\n",
        "\n",
        "# ====================================================\n",
        "# PERFORMANCE SUMMARY\n",
        "# ====================================================\n",
        "print(\"\\n============================\")\n",
        "print(\"MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"============================\")\n",
        "print(f\"Logistic Regression Accuracy : {acc_lr:.4f}\")\n",
        "print(f\"Random Forest Accuracy       : {acc_rf:.4f}\")\n",
        "print(f\"AdaBoost Accuracy            : {acc_ada:.4f}\")\n",
        "print(f\"XGBoost Accuracy             : {acc_xgb:.4f}\")\n",
        "print(\"============================\")\n",
        "\n",
        "# ====================================================\n",
        "# FINAL PREDICTIONS (use best_xgb as final model)\n",
        "# ====================================================\n",
        "final_preds = best_xgb.predict(X_test_scaled)\n",
        "final_labels = le_target.inverse_transform(final_preds)\n",
        "submission = pd.DataFrame({'id': df_test['id'], 'WeightCategory': final_labels})\n",
        "submission.to_csv(\"final_xgb_predictions.csv\", index=False)\n",
        "print(\"\\nSaved: final_xgb_predictions.csv\")\n",
        "\n",
        "# ====================================================\n",
        "# BAR CHART: Model Accuracy Comparison (kept as original)\n",
        "# ====================================================\n",
        "model_names = ['Logistic Regression', 'Random Forest', 'AdaBoost', 'XGBoost']\n",
        "accuracies = [acc_lr, acc_rf, acc_ada, acc_xgb]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=model_names, y=accuracies, palette='viridis')\n",
        "plt.title(\"Model Accuracy Comparison\", fontsize=14, weight='bold')\n",
        "plt.ylabel(\"Accuracy Score\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylim(0, 1)\n",
        "for i, acc in enumerate(accuracies):\n",
        "    plt.text(i, acc + 0.01, f\"{acc:.3f}\", ha='center', fontsize=10, weight='bold')\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# END OF SCRIPT\n",
        "# ============================================"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-27T19:20:06.106364Z",
          "iopub.execute_input": "2025-10-27T19:20:06.106755Z",
          "iopub.status.idle": "2025-10-27T20:23:56.608594Z",
          "shell.execute_reply.started": "2025-10-27T19:20:06.10673Z",
          "shell.execute_reply": "2025-10-27T20:23:56.607598Z"
        },
        "id": "meWJQLtkXC0N"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}